{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPgDhVsodeiz"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYBrPTEicI-3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Ensure you've set the HF_TOKEN environment variable with your Hugging Face token\n",
        "model_id = \"google/gemma-2b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "os.environ['HF_TOKEN'] = ''\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=os.environ['HF_TOKEN'])\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_auth_token=os.environ['HF_TOKEN'])\n",
        "\n",
        "# # If you have a GPU available\n",
        "# if torch.cuda.is_available():\n",
        "#     model.cuda()  # This moves your model to the default GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqb4ZRhvid0i"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load and select the first 100 examples\n",
        "full_dataset = load_dataset(\"tasksource/Boardgame-QA\")\n",
        "subset_dataset = full_dataset['train'].select(range(100))  # Adjust based on your dataset's structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_6Ac5jcjeCJ"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_generation(examples):\n",
        "    # Assuming 'explanation' is a field in your dataset, adjust if it's named differently\n",
        "    input_texts = examples['example']  # Directly use 'explanation' as input\n",
        "\n",
        "    # Proofs as targets\n",
        "    target_texts = examples['proof']\n",
        "\n",
        "    # Tokenize the input and target texts\n",
        "    model_inputs = tokenizer(input_texts, max_length=512, padding=\"max_length\", truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(target_texts, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrhCYCymBd7o"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "# Split the dataset\n",
        "split_dataset = subset_dataset.train_test_split(test_size=0.1)  # Adjust test_size as needed\n",
        "train_dataset1 = split_dataset['train']\n",
        "test_dataset1 = split_dataset['test']\n",
        "def tokenize_function(examples):\n",
        "    # Combine context into input text and proof as target\n",
        "    input_texts = [\"Your input construction logic here\" for example in examples['example']]  # Adjust accordingly\n",
        "    target_texts = examples['proof']  # Assuming proof is the target\n",
        "\n",
        "    model_inputs = tokenizer(input_texts, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(target_texts, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "train_dataset = train_dataset1.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset1.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "batch_size = 2  # or whatever batch size is appropriate for your setup\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m97TKoYqNqE"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):  # Adjust the number of epochs as needed\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TKuT-dTqVrg",
        "outputId": "daac5871-d847-4eba-fb51-a74e7ac667e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A few players are playing a boardgame. The current state of the game is as follows. The pelikan assassinated the mayor, and is currently in Nigeria. And the rules of the game are as follows. Rule1: One of the rules of the game is that if the pelikan does not neglect the crow, then the crow will, without hesitation, manage to convince the husky. Rule2: Here is an important piece of information about the pelikan: if it is in Africa at the moment then it does not neglect the crow for sure. Rule3: If the pelikan voted for the mayor, then the pelikan does not neglect the crow. Based on the game state and the rules and preferences, does the crow manage to convince the husky? maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc maroc\n"
          ]
        }
      ],
      "source": [
        "new_example = \"A few players are playing a boardgame. The current state of the game is as follows. The pelikan assassinated the mayor, and is currently in Nigeria. And the rules of the game are as follows. Rule1: One of the rules of the game is that if the pelikan does not neglect the crow, then the crow will, without hesitation, manage to convince the husky. Rule2: Here is an important piece of information about the pelikan: if it is in Africa at the moment then it does not neglect the crow for sure. Rule3: If the pelikan voted for the mayor, then the pelikan does not neglect the crow. Based on the game state and the rules and preferences, does the crow manage to convince the husky?\"\n",
        "input_ids = tokenizer.encode(new_example, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generating the proof\n",
        "generated_ids = model.generate(input_ids, max_length=512)\n",
        "generated_proof = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_proof)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc1VAyLmEBY7"
      },
      "outputs": [],
      "source": [
        "# Placeholder for storing example and generated proof pairs\n",
        "generated_proofs = []\n",
        "\n",
        "# Assuming 'test_dataset' is already prepared and tokenized\n",
        "for example in test_dataset1:\n",
        "    # Prepare the input for the model\n",
        "    input_ids = tokenizer.encode(example['example'], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate proof\n",
        "    generated_ids = model.generate(input_ids, max_length=1000)\n",
        "    generated_proof = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Store the results\n",
        "    generated_proofs.append({\"example\": example['example'], \"generated_proof\": generated_proof,\"label\": example['label']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_MdonkrIVVV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the results to a DataFrame\n",
        "df = pd.DataFrame(generated_proofs)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"generated_proofs_ft.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
