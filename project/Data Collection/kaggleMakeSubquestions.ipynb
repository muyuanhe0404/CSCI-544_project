{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on kaggle\n",
    "# add input of LLAMA2 13b-chat-hf\n",
    "# add DaSLAM adapter as input with name `daslam/`\n",
    "# add csv containing the file you wnat subquestions for under `data/``\n",
    "# in Session, mark Persistence (File Only) and Accelerator (GPU T4 x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub\n",
    "!pip install git+https://github.com/huggingface/transformers.git -q -U\n",
    "!pip install git+https://github.com/huggingface/accelerate.git -q -U \n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install peft\n",
    "!pip install datasets==2.16.0\n",
    "!pip install trl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall wandb --yes\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git@main\n",
    "!pip install --upgrade bitsandbytes datasets accelerate loralib\n",
    "!pip install --upgrade git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if LLAMA model was sucessfully added, you should see output here\n",
    "!ls -lh /kaggle/input/llama-2/pytorch/13b-chat-hf/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /kaggle/input/gemini/Gemini_base_results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, config=config)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, config=config, device_map=\"auto\", quantization_config=quantization_config)\n",
    "model = PeftModel.from_pretrained(model, \"/kaggle/input/daslam/DaSLaM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/input/gemini/Gemini_base_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x = 233+116+20+362+126\n",
    "subset_df = df.iloc[x:]\n",
    "#233 + 116+, 20+362,126\n",
    "\n",
    "OUTPUT_DIR = '/kaggle/working/output-gemini.json'\n",
    "\n",
    "saveDf = None#pd.read_json(OUTPUT_DIR, dtype=str)\n",
    "\n",
    "for index, row in tqdm(subset_df.iterrows(), total=len(subset_df)):\n",
    "    prompt = f'''\n",
    "                Below is an instruction that describes a task, paired with an input and a reasoning that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "                    ### Instruction: Break the input question into multiple subquestions based on the reasoning provided. Don't answer any of the questions.\n",
    "    \n",
    "                    ### Input:  {row['Example']}\n",
    "\n",
    "                    ### Reasoning: {row['Response']}\n",
    "                    \n",
    "                    ### Response: \n",
    "                '''\n",
    "\n",
    "    token_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    token_outputs = model.generate(input_ids=token_inputs['input_ids'], max_new_tokens=500)\n",
    "    \n",
    "    modelOutput = tokenizer.decode(token_outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    if saveDf is None:\n",
    "            saveDf = pd.DataFrame(columns=['question','subproblems', 'gemini-base-answer', 'label'])\n",
    "            saveDf = saveDf.astype(str)\n",
    "    else:\n",
    "        saveDf = pd.read_json(OUTPUT_DIR, dtype=str)\n",
    "    \n",
    "    split_string = modelOutput.split('### Response:')\n",
    "\n",
    "    # Take the second part (index 1) if it exists\n",
    "    if len(split_string) > 1:\n",
    "        modelOutput = split_string[1]\n",
    "\n",
    "    \n",
    "    new_record = {\n",
    "            'question': row['Example'],\n",
    "            'subproblems': modelOutput,\n",
    "            'gemini-base-answer': row['Response'],\n",
    "            'label': '',\n",
    "        }\n",
    "    \n",
    "    temp_df = pd.DataFrame([new_record])\n",
    "    saveDf = pd.concat([saveDf, temp_df], ignore_index=True)\n",
    "    saveDf.to_json(OUTPUT_DIR, orient='records')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
