{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.16.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\champ\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\champ\\anaconda3\\lib\\site-packages (from openai) (1.10.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\champ\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\champ\\anaconda3\\lib\\site-packages (from openai) (4.64.0)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\champ\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\champ\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2021.10.8)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\champ\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\champ\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.4)\n",
      "Downloading openai-1.16.1-py3-none-any.whl (266 kB)\n",
      "   ---------------------------------------- 266.9/266.9 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 75.6/75.6 kB ? eta 0:00:00\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 77.9/77.9 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, httpcore, distro, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.1.1\n",
      "    Uninstalling typing_extensions-4.1.1:\n",
      "      Successfully uninstalled typing_extensions-4.1.1\n",
      "Successfully installed distro-1.9.0 httpcore-1.0.5 httpx-0.27.0 openai-1.16.1 typing-extensions-4.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:08:13<00:00,  4.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Assuming the OpenAI and other necessary imports and setup from your initial code...\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "OPENAI_API_KEY=''\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def getChatGPT35Response(systemInstructions, prompt):\n",
    "    # sleep to avoid api limits, should take ~15 minutes in total\n",
    "    time.sleep(1)\n",
    "    #return 'something'\n",
    "    completion = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"{systemInstructions}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def getLLMResponse(systemInstructions, prompt):\n",
    "    return getChatGPT35Response(systemInstructions, prompt)\n",
    "\n",
    "def getBaseAnswers(dataDf):\n",
    "    BASE_ANS_DIR = './results.json'\n",
    "\n",
    "    saveDf = None#pd.DataFrame(columns=['question', 'reasoning', 'subproblems', 'label'])\n",
    "    #saveDf = #saveDf.astype(str)\n",
    "    \n",
    "    for index, row in tqdm(dataDf.iterrows(), total=len(dataDf)):\n",
    "        systemInstructions = \"You are a helpful bot that can answer reasoning questions based off board game sitations\"\n",
    "        prompt = row['example'] + \"\\n The label is what (proved, disproved, unknown)?\"\n",
    "        response = getLLMResponse(systemInstructions, prompt)\n",
    "        if saveDf is None:\n",
    "            saveDf = pd.DataFrame(columns=['question', 'reasoning', 'subproblems', 'label'])\n",
    "            saveDf = saveDf.astype(str)\n",
    "        else:\n",
    "            saveDf = pd.read_json(BASE_ANS_DIR, dtype=str)\n",
    "\n",
    "        new_record = {\n",
    "            'question': row['example'],\n",
    "            'gold-reasoning': row['proof'],\n",
    "            'subproblems': '',\n",
    "            'chat-gpt-3.5-turbo-ans': response,\n",
    "            'label': row['label'],\n",
    "        }\n",
    "        temp_df = pd.DataFrame([new_record])\n",
    "        saveDf = pd.concat([saveDf, temp_df], ignore_index=True)\n",
    "        saveDf.to_json(BASE_ANS_DIR, orient='records')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createCOTPrompt(facts, rules, example, proof, goal, theory):\n",
    "    \"\"\"\n",
    "    Construct a chain of thought prompt using the example and proof from the data.\n",
    "    \"\"\"\n",
    "    # The prompt starts with a breakdown of the problem into facts and rules\n",
    "    prompt = (\n",
    "        f\"Here are the facts: {facts}\\n\"\n",
    "        f\"And here are the rules: {rules}\\n\"\n",
    "        f\"Based on this, let's think through if {goal} is true.\\n\\n\"\n",
    "        f\"{example}\\n\"\n",
    "    )\n",
    "    \n",
    "    # The chain of thought is guided by the proof, ending with the goal\n",
    "    cot = (\n",
    "        f\"Let's start with the chain of thought:\\n{proof}\\n\\n\"\n",
    "        f\"Now, given this chain of thought, let's answer the question: does {goal}?\"\n",
    "    )\n",
    "    \n",
    "    return prompt + cot\n",
    "\n",
    "def getBaseAnswersWithCOT(dataDf):\n",
    "    BASE_ANS_DIR = './results_chain_of_thought.json'\n",
    "    saveDf = pd.DataFrame(columns=['question', 'gold-reasoning', 'label', 'chain_of_thought_response'])\n",
    "\n",
    "    for index, row in tqdm(dataDf.iterrows(), total=dataDf.shape[0]):\n",
    "        systemInstructions = \"You are a helpful assistant capable of reasoning through complex problems.\"\n",
    "        # Constructing the COT prompt with the example and proof from the data\n",
    "        cot_prompt = createCOTPrompt(\n",
    "            row['facts'], row['rules'], row['example'], row['proof'], row['goal'], row['theory']\n",
    "        )\n",
    "        response = getLLMResponse(systemInstructions, cot_prompt)\n",
    "\n",
    "        # Create a new record for saving the response with chain of thought\n",
    "        new_record = {\n",
    "            'question': row['example'],\n",
    "            'gold-reasoning': row['proof'],\n",
    "            'label': row['label'],\n",
    "            'chain_of_thought_response': response,\n",
    "        }\n",
    "        temp_df = pd.DataFrame([new_record])\n",
    "        saveDf = pd.concat([saveDf, temp_df], ignore_index=True)\n",
    "\n",
    "        # Sleep to avoid hitting API rate limits, if needed\n",
    "        time.sleep(1)\n",
    "\n",
    "    saveDf.to_json(BASE_ANS_DIR, orient='records', force_ascii=False)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the dataset\n",
    "    dataPath = \"../data/test.json\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_json(dataPath, dtype=str)\n",
    "\n",
    "    # Run the COT prompting process\n",
    "    getBaseAnswersWithCOT(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
